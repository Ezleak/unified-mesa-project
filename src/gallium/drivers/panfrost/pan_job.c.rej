diff a/src/gallium/drivers/panfrost/pan_job.c b/src/gallium/drivers/panfrost/pan_job.c	(rejected hunks)
@@ -25,6 +25,7 @@
  */
 
 #include <assert.h>
+#include <unistd.h>
 
 #include "drm-uapi/panfrost_drm.h"
 
@@ -81,6 +82,14 @@ panfrost_batch_init(struct panfrost_context *ctx,
         batch->resources =_mesa_set_create(NULL, _mesa_hash_pointer,
                                           _mesa_key_pointer_equal);
 
+        for (unsigned i = 0; i < PAN_USAGE_COUNT; ++i)
+                util_dynarray_init(&batch->resource_bos[i], NULL);
+
+        util_dynarray_init(&batch->vert_deps, NULL);
+        util_dynarray_init(&batch->frag_deps, NULL);
+
+        util_dynarray_init(&batch->dmabufs, NULL);
+
         /* Preallocate the main pool, since every batch has at least one job
          * structure so it will be used */
         panfrost_pool_init(&batch->pool, NULL, dev, 0, 65536, "Batch pool", true, true);
@@ -96,6 +105,9 @@ panfrost_batch_init(struct panfrost_context *ctx,
 
         panfrost_batch_add_surface(batch, batch->key.zsbuf);
 
+        if ((dev->debug & PAN_DBG_SYNC) || !(dev->debug & PAN_DBG_GOFASTER))
+                batch->needs_sync = true;
+
         screen->vtbl.init_batch(batch);
 }
 
@@ -115,15 +127,30 @@ static void
 panfrost_batch_add_resource(struct panfrost_batch *batch,
                             struct panfrost_resource *rsrc)
 {
+        struct panfrost_context *ctx = batch->ctx;
+        struct panfrost_device *dev = pan_device(ctx->base.screen);
+
         bool found = false;
         _mesa_set_search_or_add(batch->resources, rsrc, &found);
 
-        if (!found) {
-                /* Cache number of batches accessing a resource */
-                rsrc->track.nr_users++;
+        /* Nothing to do if we already have the resource */
+        if (found)
+                return;
+
+        /* Cache number of batches accessing a resource */
+        rsrc->track.nr_users++;
+
+        /* Reference the resource on the batch */
+        pipe_reference(NULL, &rsrc->base.reference);
 
-                /* Reference the resource on the batch */
-                pipe_reference(NULL, &rsrc->base.reference);
+        if (rsrc->scanout) {
+                if (dev->has_dmabuf_fence) {
+                        int fd = rsrc->image.data.bo->dmabuf_fd;
+                        util_dynarray_append(&batch->dmabufs, int, fd);
+                } else {
+                        perf_debug_ctx(ctx, "Forcing sync on batch");
+                        batch->needs_sync = true;
+                }
         }
 }
 
@@ -172,6 +199,10 @@ panfrost_batch_cleanup(struct panfrost_context *ctx, struct panfrost_batch *batc
 {
         struct panfrost_device *dev = pan_device(ctx->base.screen);
 
+        /* Make sure we keep handling events, to free old BOs */
+        if (dev->kbase)
+                kbase_ensure_handle_events(&dev->mali);
+
         assert(batch->seqnum);
 
         if (ctx->batch == batch)
@@ -186,10 +217,18 @@ panfrost_batch_cleanup(struct panfrost_context *ctx, struct panfrost_batch *batc
                 if (!flags[i])
                         continue;
 
-                struct panfrost_bo *bo = pan_lookup_bo(dev, i);
+                struct panfrost_bo *bo = pan_lookup_bo_existing(dev, i);
                 panfrost_bo_unreference(bo);
         }
 
+        util_dynarray_fini(&batch->dmabufs);
+
+        util_dynarray_fini(&batch->vert_deps);
+        util_dynarray_fini(&batch->frag_deps);
+
+        for (unsigned i = 0; i < PAN_USAGE_COUNT; ++i)
+                util_dynarray_fini(&batch->resource_bos[i]);
+
         panfrost_batch_destroy_resources(ctx, batch);
         panfrost_pool_cleanup(&batch->pool);
         panfrost_pool_cleanup(&batch->invisible_pool);
@@ -313,7 +352,7 @@ panfrost_batch_update_access(struct panfrost_batch *batch,
                 }
         }
 
-        if (writes) {
+        if (writes && (writer != batch)) {
                 _mesa_hash_table_insert(ctx->writers, rsrc, batch);
                 rsrc->track.nr_writers++;
         }
@@ -380,6 +419,12 @@ panfrost_batch_read_rsrc(struct panfrost_batch *batch,
         uint32_t access = PAN_BO_ACCESS_READ |
                 panfrost_access_for_stage(stage);
 
+        enum panfrost_usage_type type = (stage == MESA_SHADER_FRAGMENT) ?
+                PAN_USAGE_READ_FRAGMENT : PAN_USAGE_READ_VERTEX;
+
+        util_dynarray_append(&batch->resource_bos[type], struct panfrost_bo *,
+                             rsrc->image.data.bo);
+
         panfrost_batch_add_bo_old(batch, rsrc->image.data.bo, access);
 
         if (rsrc->separate_stencil)
@@ -396,6 +441,12 @@ panfrost_batch_write_rsrc(struct panfrost_batch *batch,
         uint32_t access = PAN_BO_ACCESS_WRITE |
                 panfrost_access_for_stage(stage);
 
+        enum panfrost_usage_type type = (stage == MESA_SHADER_FRAGMENT) ?
+                PAN_USAGE_WRITE_FRAGMENT : PAN_USAGE_WRITE_VERTEX;
+
+        util_dynarray_append(&batch->resource_bos[type], struct panfrost_bo *,
+                             rsrc->image.data.bo);
+
         panfrost_batch_add_bo_old(batch, rsrc->image.data.bo, access);
 
         if (rsrc->separate_stencil)
@@ -489,7 +540,7 @@ panfrost_batch_get_shared_memory(struct panfrost_batch *batch,
 }
 
 static void
-panfrost_batch_to_fb_info(const struct panfrost_batch *batch,
+panfrost_batch_to_fb_info(struct panfrost_batch *batch,
                           struct pan_fb_info *fb,
                           struct pan_image_view *rts,
                           struct pan_image_view *zs,
@@ -511,6 +562,7 @@ panfrost_batch_to_fb_info(const struct panfrost_batch *batch,
         fb->rt_count = batch->key.nr_cbufs;
         fb->sprite_coord_origin = pan_tristate_get(batch->sprite_coord_origin);
         fb->first_provoking_vertex = pan_tristate_get(batch->first_provoking_vertex);
+        fb->cs_fragment = &batch->cs_fragment;
 
         static const unsigned char id_swz[] = {
                 PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W,
@@ -604,22 +656,22 @@ panfrost_batch_to_fb_info(const struct panfrost_batch *batch,
         fb->zs.discard.z = !reserve && !(batch->resolve & PIPE_CLEAR_DEPTH);
         fb->zs.discard.s = !reserve && !(batch->resolve & PIPE_CLEAR_STENCIL);
 
-        if (!fb->zs.clear.z &&
+        if (!fb->zs.clear.z && z_rsrc &&
             ((batch->read & PIPE_CLEAR_DEPTH) ||
              ((batch->draws & PIPE_CLEAR_DEPTH) &&
-              z_rsrc && BITSET_TEST(z_rsrc->valid.data, z_view->first_level))))
+              BITSET_TEST(z_rsrc->valid.data, z_view->first_level))))
                 fb->zs.preload.z = true;
 
-        if (!fb->zs.clear.s &&
+        if (!fb->zs.clear.s && s_rsrc &&
             ((batch->read & PIPE_CLEAR_STENCIL) ||
              ((batch->draws & PIPE_CLEAR_STENCIL) &&
-              s_rsrc && BITSET_TEST(s_rsrc->valid.data, s_view->first_level))))
+              BITSET_TEST(s_rsrc->valid.data, s_view->first_level))))
                 fb->zs.preload.s = true;
 
         /* Preserve both component if we have a combined ZS view and
          * one component needs to be preserved.
          */
-        if (s_view == z_view && fb->zs.discard.z != fb->zs.discard.s) {
+        if (z_view && s_view == z_view && fb->zs.discard.z != fb->zs.discard.s) {
                 bool valid = BITSET_TEST(z_rsrc->valid.data, z_view->first_level);
 
                 fb->zs.discard.z = false;
@@ -629,6 +681,28 @@ panfrost_batch_to_fb_info(const struct panfrost_batch *batch,
         }
 }
 
+static int
+panfrost_batch_submit_kbase(struct panfrost_device *dev,
+                            struct drm_panfrost_submit *submit,
+                            struct kbase_syncobj *syncobj)
+{
+        dev->mali.handle_events(&dev->mali);
+
+        int atom = dev->mali.submit(&dev->mali,
+                                    submit->jc,
+                                    submit->requirements,
+                                    syncobj,
+                                    (int32_t *)(uintptr_t) submit->bo_handles,
+                                    submit->bo_handle_count);
+
+        if (atom == -1) {
+                errno = EINVAL;
+                return -1;
+        }
+
+        return 0;
+}
+
 static int
 panfrost_batch_submit_ioctl(struct panfrost_batch *batch,
                             mali_ptr first_job_desc,
@@ -695,7 +769,7 @@ panfrost_batch_submit_ioctl(struct panfrost_batch *batch,
                  * We also preserve existing flags as this batch might not
                  * be the first one to access the BO.
                  */
-                struct panfrost_bo *bo = pan_lookup_bo(dev, i);
+                struct panfrost_bo *bo = pan_lookup_bo_existing(dev, i);
 
                 bo->gpu_access |= flags[i] & (PAN_BO_ACCESS_RW);
         }
@@ -718,6 +792,8 @@ panfrost_batch_submit_ioctl(struct panfrost_batch *batch,
         submit.bo_handles = (u64) (uintptr_t) bo_handles;
         if (ctx->is_noop)
                 ret = 0;
+        else if (dev->kbase)
+                ret = panfrost_batch_submit_kbase(dev, &submit, ctx->syncobj_kbase);
         else
                 ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
         free(bo_handles);
@@ -728,8 +804,11 @@ panfrost_batch_submit_ioctl(struct panfrost_batch *batch,
         /* Trace the job if we're doing that */
         if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
                 /* Wait so we can get errors reported back */
-                drmSyncobjWait(dev->fd, &out_sync, 1,
-                               INT64_MAX, 0, NULL);
+                if (dev->kbase)
+                        dev->mali.syncobj_wait(&dev->mali, ctx->syncobj_kbase);
+                else
+                        drmSyncobjWait(dev->fd, &out_sync, 1,
+                                       INT64_MAX, 0, NULL);
 
                 if (dev->debug & PAN_DBG_TRACE)
                         pandecode_jc(submit.jc, dev->gpu_id);
@@ -799,6 +878,323 @@ panfrost_batch_submit_jobs(struct panfrost_batch *batch,
         return ret;
 }
 
+#define BASE_MEM_MMU_DUMP_HANDLE (1 << 12)
+
+static void
+mmu_dump(struct panfrost_device *dev)
+{
+        unsigned size = 16 * 1024 * 1024;
+
+        fprintf(stderr, "dumping MMU tables\n");
+        sleep(3);
+
+        void *mem = mmap(NULL, size, PROT_READ, MAP_SHARED,
+                         dev->mali.fd, BASE_MEM_MMU_DUMP_HANDLE);
+        if (mem == MAP_FAILED) {
+                perror("mmap(BASE_MEM_MMU_DUMP_HANDLE)");
+                return;;
+        }
+
+        fprintf(stderr, "writing to file\n");
+        sleep(1);
+
+        char template[] = {"/tmp/mmu-dump.XXXXXX"};
+        int fd = mkstemp(template);
+        if (fd == -1) {
+                perror("mkstemp(/tmp/mmu-dump.XXXXXX)");
+                goto unmap;
+        }
+
+        write(fd, mem, size);
+        close(fd);
+
+unmap:
+        munmap(mem, size);
+}
+
+static void
+reset_context(struct panfrost_context *ctx)
+{
+        struct pipe_screen *pscreen = ctx->base.screen;
+        struct panfrost_screen *screen = pan_screen(pscreen);
+        struct panfrost_device *dev = pan_device(pscreen);
+
+        /* Don't recover from the fault if PAN_MESA_DEBUG=sync is specified,
+         * to somewhat mimic behaviour with JM GPUs. TODO: Just abort? */
+        bool recover = !(dev->debug & PAN_DBG_SYNC);
+
+        mesa_loge("Context reset");
+
+        dev->mali.cs_term(&dev->mali, &ctx->kbase_cs_vertex.base);
+        dev->mali.cs_term(&dev->mali, &ctx->kbase_cs_fragment.base);
+
+        dev->mali.context_recreate(&dev->mali, ctx->kbase_ctx);
+
+        //mmu_dump(dev);
+
+        if (recover) {
+                dev->mali.cs_rebind(&dev->mali, &ctx->kbase_cs_vertex.base);
+                dev->mali.cs_rebind(&dev->mali, &ctx->kbase_cs_fragment.base);
+        } else {
+                ctx->kbase_cs_vertex.base.user_io = NULL;
+                ctx->kbase_cs_fragment.base.user_io = NULL;
+        }
+
+        ctx->kbase_cs_vertex.base.last_insert = 0;
+        ctx->kbase_cs_fragment.base.last_insert = 0;
+
+        screen->vtbl.init_cs(ctx, &ctx->kbase_cs_vertex);
+        screen->vtbl.init_cs(ctx, &ctx->kbase_cs_fragment);
+
+        /* TODO: this leaks memory */
+        ctx->tiler_heap_desc = 0;
+}
+
+static void
+pandecode_cs_ring(struct panfrost_device *dev, struct panfrost_cs *cs,
+                  uint64_t insert)
+{
+        insert %= cs->base.size;
+        uint64_t start = cs->base.last_insert % cs->base.size;
+
+        if (insert < start) {
+                pandecode_cs(cs->base.va + start, cs->base.size - start, dev->gpu_id);
+                start = 0;
+        }
+
+        pandecode_cs(cs->base.va + start, insert - start, dev->gpu_id);
+}
+
+static unsigned
+panfrost_add_dep_after(struct util_dynarray *deps,
+                       struct panfrost_usage u,
+                       unsigned index)
+{
+        unsigned size = util_dynarray_num_elements(deps, struct panfrost_usage);
+
+        for (unsigned i = index; i < size; ++i) {
+                struct panfrost_usage *d =
+                        util_dynarray_element(deps, struct panfrost_usage, i);
+
+                /* TODO: Remove d if it is an invalid entry? */
+
+                if ((d->queue == u.queue) && (d->write == u.write)) {
+                        d->seqnum = MAX2(d->seqnum, u.seqnum);
+                        return i;
+
+                } else if (d->queue > u.queue) {
+                        void *p = util_dynarray_grow(deps, struct panfrost_usage, 1);
+                        assert(p);
+                        memmove(util_dynarray_element(deps, struct panfrost_usage, i + 1),
+                                util_dynarray_element(deps, struct panfrost_usage, i),
+                                (size - i) * sizeof(struct panfrost_usage));
+
+                        *util_dynarray_element(deps, struct panfrost_usage, i) = u;
+                        return i;
+                }
+        }
+
+        util_dynarray_append(deps, struct panfrost_usage, u);
+        return size;
+}
+
+static void
+panfrost_update_deps(struct util_dynarray *deps, struct panfrost_bo *bo, bool write)
+{
+        /* Both lists should be sorted, so each dependency is at a higher
+         * index than the last */
+        unsigned index = 0;
+        util_dynarray_foreach(&bo->usage, struct panfrost_usage, u) {
+                /* read->read access does not require a dependency */
+                if (!write && !u->write)
+                        continue;
+
+                index = panfrost_add_dep_after(deps, *u, index);
+        }
+}
+
+static inline bool
+panfrost_usage_writes(enum panfrost_usage_type usage)
+{
+        return (usage == PAN_USAGE_WRITE_VERTEX) || (usage == PAN_USAGE_WRITE_FRAGMENT);
+}
+
+static inline bool
+panfrost_usage_fragment(enum panfrost_usage_type usage)
+{
+        return (usage == PAN_USAGE_READ_FRAGMENT) || (usage == PAN_USAGE_WRITE_FRAGMENT);
+}
+
+/* Removes invalid dependencies from deps */
+static void
+panfrost_clean_deps(struct panfrost_device *dev, struct util_dynarray *deps)
+{
+        kbase k = &dev->mali;
+
+        struct panfrost_usage *rebuild = util_dynarray_begin(deps);
+        unsigned index = 0;
+
+        util_dynarray_foreach(deps, struct panfrost_usage, u) {
+                /* Usages are ordered, so we can break here */
+                if (u->queue >= k->event_slot_usage)
+                        break;
+
+                struct kbase_event_slot *slot = &k->event_slots[u->queue];
+                uint64_t seqnum = u->seqnum;
+
+                /* There is a race condition, where we can depend on an
+                 * unsubmitted batch. In that cade, decrease the seqnum.
+                 * Otherwise, skip invalid dependencies. */
+                if (slot->last_submit == seqnum)
+                        --seqnum;
+                else if (slot->last_submit < seqnum)
+                        continue;
+
+                /* This usage is valid, add it to the returned list */
+                rebuild[index++] = (struct panfrost_usage) {
+                        .queue = u->queue,
+                        .write = u->write,
+                        .seqnum = seqnum,
+                };
+        }
+
+        /* No need to check the return value, it can only shrink */
+        (void)! util_dynarray_resize(deps, struct panfrost_usage, index);
+}
+
+static int
+panfrost_batch_submit_csf(struct panfrost_batch *batch,
+                          const struct pan_fb_info *fb)
+{
+        struct panfrost_context *ctx = batch->ctx;
+        struct pipe_screen *pscreen = ctx->base.screen;
+        struct panfrost_screen *screen = pan_screen(pscreen);
+        struct panfrost_device *dev = pan_device(pscreen);
+
+        ++ctx->kbase_cs_vertex.seqnum;
+
+        if (panfrost_has_fragment_job(batch)) {
+                screen->vtbl.emit_fragment_job(batch, fb);
+                ++ctx->kbase_cs_fragment.seqnum;
+        }
+
+        pthread_mutex_lock(&dev->bo_usage_lock);
+        for (unsigned i = 0; i < PAN_USAGE_COUNT; ++i) {
+
+                bool write = panfrost_usage_writes(i);
+                pan_bo_access access = write ? PAN_BO_ACCESS_RW : PAN_BO_ACCESS_READ;
+                struct util_dynarray *deps;
+                unsigned queue;
+                uint64_t seqnum;
+
+                if (panfrost_usage_fragment(i)) {
+                        deps = &batch->frag_deps;
+                        queue = ctx->kbase_cs_fragment.base.event_mem_offset;
+                        seqnum = ctx->kbase_cs_fragment.seqnum;
+                } else {
+                        deps = &batch->vert_deps;
+                        queue = ctx->kbase_cs_vertex.base.event_mem_offset;
+                        seqnum = ctx->kbase_cs_vertex.seqnum;
+                }
+
+                util_dynarray_foreach(&batch->resource_bos[i], struct panfrost_bo *, bo) {
+                        panfrost_update_deps(deps, *bo, write);
+                        struct panfrost_usage u = {
+                                .queue = queue,
+                                .write = write,
+                                .seqnum = seqnum,
+                        };
+
+                        panfrost_add_dep_after(&(*bo)->usage, u, 0);
+                        (*bo)->gpu_access |= access;
+                }
+        }
+        pthread_mutex_unlock(&dev->bo_usage_lock);
+
+        /* For now, only a single batch can use each tiler heap at once */
+        if (ctx->tiler_heap_desc) {
+                panfrost_update_deps(&batch->vert_deps, ctx->tiler_heap_desc, true);
+
+                struct panfrost_usage u = {
+                        .queue = ctx->kbase_cs_fragment.base.event_mem_offset,
+                        .write = true,
+                        .seqnum = ctx->kbase_cs_fragment.seqnum,
+                };
+                panfrost_add_dep_after(&ctx->tiler_heap_desc->usage, u, 0);
+        }
+
+        /* TODO: Use atomics in kbase code to avoid lock? */
+        pthread_mutex_lock(&dev->mali.queue_lock);
+
+        panfrost_clean_deps(dev, &batch->vert_deps);
+        panfrost_clean_deps(dev, &batch->frag_deps);
+
+        pthread_mutex_unlock(&dev->mali.queue_lock);
+
+        screen->vtbl.emit_csf_toplevel(batch);
+
+        uint64_t vs_offset = ctx->kbase_cs_vertex.offset +
+                (void *)ctx->kbase_cs_vertex.cs.ptr - ctx->kbase_cs_vertex.bo->ptr.cpu;
+        uint64_t fs_offset = ctx->kbase_cs_fragment.offset +
+                (void *)ctx->kbase_cs_fragment.cs.ptr - ctx->kbase_cs_fragment.bo->ptr.cpu;
+
+        if (dev->debug & PAN_DBG_TRACE) {
+                pandecode_cs_ring(dev, &ctx->kbase_cs_vertex, vs_offset);
+                pandecode_cs_ring(dev, &ctx->kbase_cs_fragment, fs_offset);
+        }
+
+        bool log = (dev->debug & PAN_DBG_LOG);
+
+        // TODO: We need better synchronisation than a single fake syncobj!
+
+        if (log)
+                printf("About to submit\n");
+
+        dev->mali.cs_submit(&dev->mali, &ctx->kbase_cs_vertex.base, vs_offset,
+                            ctx->syncobj_kbase, ctx->kbase_cs_vertex.seqnum);
+
+        dev->mali.cs_submit(&dev->mali, &ctx->kbase_cs_fragment.base, fs_offset,
+                            ctx->syncobj_kbase, ctx->kbase_cs_fragment.seqnum);
+
+        bool reset = false;
+
+        // TODO: How will we know to reset a CS when waiting is not done?
+        if (batch->needs_sync) {
+                if (!dev->mali.cs_wait(&dev->mali, &ctx->kbase_cs_vertex.base, vs_offset, ctx->syncobj_kbase))
+                        reset = true;
+
+                if (!dev->mali.cs_wait(&dev->mali, &ctx->kbase_cs_fragment.base, fs_offset, ctx->syncobj_kbase))
+                        reset = true;
+        }
+
+        if (dev->debug & PAN_DBG_TILER) {
+                fflush(stdout);
+                FILE *stream = popen("tiler-hex-read", "w");
+
+                /* TODO: Dump more than just the first chunk */
+                unsigned size = batch->ctx->kbase_ctx->tiler_heap_chunk_size;
+                uint64_t va = batch->ctx->kbase_ctx->tiler_heap_header;
+
+                fprintf(stream, "width %i\n" "height %i\n" "mask %i\n"
+                        "vaheap 0x%"PRIx64"\n" "size %i\n",
+                        batch->key.width, batch->key.height, 0xfe, va, size);
+
+                void *ptr = mmap(NULL, size, PROT_READ | PROT_WRITE,
+                                 MAP_SHARED, dev->mali.fd, va);
+
+                pan_hexdump(stream, ptr, size, false);
+                //memset(ptr, 0, size);
+                munmap(ptr, size);
+
+                pclose(stream);
+        }
+
+        if (reset)
+                reset_context(ctx);
+
+        return 0;
+}
+
 static void
 panfrost_emit_tile_map(struct panfrost_batch *batch, struct pan_fb_info *fb)
 {
@@ -824,6 +1220,7 @@ panfrost_batch_submit(struct panfrost_context *ctx,
 {
         struct pipe_screen *pscreen = ctx->base.screen;
         struct panfrost_screen *screen = pan_screen(pscreen);
+        struct panfrost_device *dev = pan_device(pscreen);
         int ret;
 
         /* Nothing to do! */
@@ -867,7 +1264,11 @@ panfrost_batch_submit(struct panfrost_context *ctx,
         if (batch->scoreboard.first_tiler || batch->clear)
                 screen->vtbl.emit_fbd(batch, &fb);
 
-        ret = panfrost_batch_submit_jobs(batch, &fb, 0, ctx->syncobj);
+        /* TODO: Don't hardcode the arch number */
+        if (dev->arch < 10)
+                ret = panfrost_batch_submit_jobs(batch, &fb, 0, ctx->syncobj);
+        else
+                ret = panfrost_batch_submit_csf(batch, &fb);
 
         if (ret)
                 fprintf(stderr, "panfrost_batch_submit failed: %d\n", ret);
@@ -969,6 +1370,8 @@ panfrost_batch_clear(struct panfrost_batch *batch,
                 for (unsigned i = 0; i < ctx->pipe_framebuffer.nr_cbufs; ++i) {
                         if (!(buffers & (PIPE_CLEAR_COLOR0 << i)))
                                 continue;
+                        if (!ctx->pipe_framebuffer.cbufs[i])
+                                continue;
 
                         enum pipe_format format = ctx->pipe_framebuffer.cbufs[i]->format;
                         pan_pack_color(batch->clear_color[i], color, format, false);
