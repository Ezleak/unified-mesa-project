diff a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c	(rejected hunks)
@@ -23,12 +23,15 @@
  * SOFTWARE.
  */
 
+#include "dma-uapi/dma-buf.h"
+
 #include "util/macros.h"
 #include "util/u_prim.h"
 #include "util/u_vbuf.h"
 #include "util/u_helpers.h"
 #include "util/u_draw.h"
 #include "util/u_memory.h"
+#include "util/u_viewport.h"
 #include "pipe/p_defines.h"
 #include "pipe/p_state.h"
 #include "gallium/auxiliary/util/u_blend.h"
@@ -749,8 +752,8 @@ panfrost_emit_viewport(struct panfrost_batch *batch)
         float vp_maxx = vp->translate[0] + fabsf(vp->scale[0]);
         float vp_miny = vp->translate[1] - fabsf(vp->scale[1]);
         float vp_maxy = vp->translate[1] + fabsf(vp->scale[1]);
-        float minz = (vp->translate[2] - fabsf(vp->scale[2]));
-        float maxz = (vp->translate[2] + fabsf(vp->scale[2]));
+        float minz, maxz;
+        util_viewport_zmin_zmax(vp, rast->clip_halfz, &minz, &maxz);
 
         /* Scissor to the intersection of viewport and to the scissor, clamped
          * to the framebuffer */
@@ -778,10 +781,16 @@ panfrost_emit_viewport(struct panfrost_batch *batch)
         maxx--;
         maxy--;
 
-        batch->minimum_z = rast->depth_clip_near ? minz : -INFINITY;
-        batch->maximum_z = rast->depth_clip_far  ? maxz : +INFINITY;
-
 #if PAN_ARCH <= 7
+        /* Proper depth clamp support was only introduced in v9, before then
+         * all that can be done is disabling clipping by adjusting the
+         * viewport. This means that the result will be wrong for float depth
+         * buffers or non-[0, 1] depth range. */
+        if (!rast->depth_clip_near)
+                minz = -INFINITY;
+        if (!rast->depth_clip_far)
+                maxz = +INFINITY;
+
         struct panfrost_ptr T = pan_pool_alloc_desc(&batch->pool.base, VIEWPORT);
 
         pan_pack(T.cpu, VIEWPORT, cfg) {
@@ -790,19 +799,22 @@ panfrost_emit_viewport(struct panfrost_batch *batch)
                 cfg.scissor_maximum_x = maxx;
                 cfg.scissor_maximum_y = maxy;
 
-                cfg.minimum_z = batch->minimum_z;
-                cfg.maximum_z = batch->maximum_z;
+                cfg.minimum_z = minz;
+                cfg.maximum_z = maxz;
         }
 
         return T.gpu;
 #else
-        pan_pack(&batch->scissor, SCISSOR, cfg) {
+        pan_pack_cs_v10(&batch->scissor, &batch->cs_vertex, SCISSOR, cfg) {
                 cfg.scissor_minimum_x = minx;
                 cfg.scissor_minimum_y = miny;
                 cfg.scissor_maximum_x = maxx;
                 cfg.scissor_maximum_y = maxy;
         }
 
+        batch->minimum_z = minz;
+        batch->maximum_z = maxz;
+
         return 0;
 #endif
 }
@@ -838,6 +850,14 @@ panfrost_emit_depth_stencil(struct panfrost_batch *batch)
                 cfg.depth_units = rast->base.offset_units * 2.0f;
                 cfg.depth_factor = rast->base.offset_scale;
                 cfg.depth_bias_clamp = rast->base.offset_clamp;
+
+                if (rast->base.depth_clip_near && rast->base.depth_clip_far) {
+                        cfg.depth_clamp_mode = MALI_DEPTH_CLAMP_MODE_0_1;
+                        cfg.depth_cull_enable = true;
+                } else {
+                        cfg.depth_clamp_mode = MALI_DEPTH_CLAMP_MODE_BOUNDS;
+                        cfg.depth_cull_enable = false;
+                }
         }
 
         pan_merge(dynamic, zsa->desc, DEPTH_STENCIL);
@@ -1482,9 +1502,17 @@ panfrost_emit_const_buf(struct panfrost_batch *batch,
         size_t sys_size = sizeof(float) * 4 * ss->info.sysvals.sysval_count;
         struct panfrost_ptr transfer =
                 pan_pool_alloc_aligned(&batch->pool.base, sys_size, 16);
+        void *sys_cpu = malloc(sys_size);
+
+        /* Write to a shadow buffer to make pushing cheaper */
+        struct panfrost_ptr sys_shadow = {
+                .cpu = sys_cpu,
+                .gpu = transfer.gpu,
+        };
 
         /* Upload sysvals requested by the shader */
-        panfrost_upload_sysvals(batch, &transfer, ss, stage);
+        panfrost_upload_sysvals(batch, &sys_shadow, ss, stage);
+        memcpy(transfer.cpu, sys_cpu, sys_size);
 
         /* Next up, attach UBOs. UBO count includes gaps but no sysval UBO */
         struct panfrost_compiled_shader *shader = ctx->prog[stage];
@@ -1527,8 +1555,10 @@ panfrost_emit_const_buf(struct panfrost_batch *batch,
         if (pushed_words)
                 *pushed_words = ss->info.push.count;
 
-        if (ss->info.push.count == 0)
+        if (ss->info.push.count == 0) {
+                free(sys_cpu);
                 return ubos.gpu;
+        }
 
         /* Copy push constants required by the shader */
         struct panfrost_ptr push_transfer =
@@ -1580,13 +1610,15 @@ panfrost_emit_const_buf(struct panfrost_batch *batch,
                  * off to upload sysvals to a staging buffer on the CPU on the
                  * assumption sysvals will get pushed (TODO) */
 
-                const void *mapped_ubo = (src.ubo == sysval_ubo) ? transfer.cpu :
+                const void *mapped_ubo = (src.ubo == sysval_ubo) ? sys_cpu :
                         panfrost_map_constant_buffer_cpu(ctx, buf, src.ubo);
 
                 /* TODO: Is there any benefit to combining ranges */
                 memcpy(push_cpu + i, (uint8_t *) mapped_ubo + src.offset, 4);
         }
 
+        free(sys_cpu);
+
         return ubos.gpu;
 }
 
@@ -2777,6 +2809,385 @@ emit_fragment_job(struct panfrost_batch *batch, const struct pan_fb_info *pfb)
         return transfer.gpu;
 }
 
+#if PAN_ARCH >= 10
+
+static int
+panfrost_export_dmabuf_fence(int dmabuf)
+{
+        struct dma_buf_export_sync_file export = {
+                .flags = DMA_BUF_SYNC_RW,
+        };
+
+        int err = drmIoctl(dmabuf, DMA_BUF_IOCTL_EXPORT_SYNC_FILE, &export);
+        if (err < 0) {
+                fprintf(stderr, "failed to export fence: %s\n",
+                        strerror(errno));
+                return -1;
+        }
+
+        return export.fd;
+}
+
+static bool
+panfrost_import_dmabuf_fence(int dmabuf, int fence)
+{
+        struct dma_buf_import_sync_file import = {
+                .flags = DMA_BUF_SYNC_RW,
+                .fd = fence,
+        };
+
+        int err = drmIoctl(dmabuf, DMA_BUF_IOCTL_IMPORT_SYNC_FILE, &import);
+        if (err < 0) {
+                fprintf(stderr, "failed to import fence: %s\n",
+                        strerror(errno));
+                return false;
+        }
+
+        return true;
+}
+
+static uint64_t *
+panfrost_cs_ring_allocate_instrs(struct panfrost_cs *cs, unsigned count)
+{
+        pan_command_stream c = cs->cs;
+
+        if (c.ptr + count > c.end) {
+                assert(c.ptr <= c.end);
+                assert(c.begin + count <= c.ptr);
+
+                /* Instructions are in a ring buffer, simply NOP out the end
+                 * and start back from the start. Possibly, doing a TAILCALL
+                 * straight to the start could also work. */
+                memset(c.ptr, 0, (c.end - c.ptr) * 8);
+                c.ptr = c.begin;
+
+                cs->offset += cs->base.size;
+                cs->cs = c;
+        }
+
+        /* TODO: Check against the extract offset */
+        return c.ptr + count;
+}
+
+// TODO: Rewrite this!
+static void
+emit_csf_queue(struct panfrost_batch *batch, struct panfrost_cs *cs,
+               pan_command_stream s, struct util_dynarray *deps,
+               bool first, bool last)
+{
+        struct panfrost_device *dev = pan_device(batch->ctx->base.screen);
+
+        assert(s.ptr <= s.end);
+
+        bool fragment = (cs->hw_resources & 2);
+        bool vertex = (cs->hw_resources & 12); /* TILER | IDVS */
+
+        uint64_t *limit = panfrost_cs_ring_allocate_instrs(cs,
+                128 + util_dynarray_num_elements(deps, struct panfrost_usage) * 4);
+
+        pan_command_stream *c = &cs->cs;
+
+        /* First, do some waiting at the start of the job */
+
+        pan_emit_cs_32(c, 0x54, *cs->base.latest_flush);
+        // TODO genxmlify
+        pan_emit_cs_ins(c, 0x24, 0x540000000233ULL);
+        // TODO: What does this need to be?
+        pan_pack_ins(c, CS_WAIT, cfg) { cfg.slots = 0xff; }
+
+        /* For the first job in the batch, wait on dependencies */
+        // TODO: Usually the vertex job shouldn't have to wait for dmabufs!
+        if (first) {
+                mali_ptr seqnum_ptr_base = dev->mali.event_mem.gpu;
+
+                util_dynarray_foreach(deps, struct panfrost_usage, u) {
+                        /* Note the multiplication in the call to
+                         * cs_ring_allocate_instrs. pan_emit_cs_64 might be
+                         * split, so the total is four instructions. */
+                        pan_emit_cs_48(c, 0x42, seqnum_ptr_base +
+                                       u->queue * PAN_EVENT_SIZE);
+                        pan_emit_cs_64(c, 0x40, u->seqnum);
+                        pan_pack_ins(c, CS_EVWAIT_64, cfg) {
+                                cfg.no_error = true;
+                                cfg.condition = MALI_WAIT_CONDITION_HIGHER;
+                                cfg.value = 0x40;
+                                cfg.addr = 0x42;
+                        }
+                }
+
+                uint64_t kcpu_seqnum = ++cs->kcpu_seqnum;
+
+                util_dynarray_foreach(&batch->dmabufs, int, fd) {
+                        int fence = panfrost_export_dmabuf_fence(*fd);
+
+                        /* TODO: poll on the dma-buf? */
+                        if (fence == -1)
+                                continue;
+
+                        // TODO: What if we reach the limit for number of KCPU
+                        // commands in a queue? It's pretty low (256)
+                        dev->mali.kcpu_fence_import(&dev->mali, cs->base.ctx,
+                                                    fence);
+
+                        close(fence);
+                }
+
+                bool ret = dev->mali.kcpu_cqs_set(&dev->mali, cs->base.ctx,
+                                  cs->kcpu_event_ptr, kcpu_seqnum + 1);
+
+                if (ret) {
+                        /* If we don't set no_error, kbase might decide to
+                         * pass on errors from waiting for fences. */
+                        pan_emit_cs_48(c, 0x42, cs->kcpu_event_ptr);
+                        pan_emit_cs_64(c, 0x40, kcpu_seqnum);
+                        pan_pack_ins(c, CS_EVWAIT_64, cfg) {
+                                cfg.no_error = true;
+                                cfg.condition = MALI_WAIT_CONDITION_HIGHER;
+                                cfg.value = 0x40;
+                                cfg.addr = 0x42;
+                        }
+                }
+        }
+
+        /* Fragment jobs need to wait for the vertex job */
+        if (fragment && !first) {
+                pan_pack_ins(c, CS_EVWAIT_64, cfg) {
+                        cfg.condition = MALI_WAIT_CONDITION_HIGHER;
+                        cfg.value = 0x4e;
+                        cfg.addr = 0x4c;
+                }
+        }
+
+        if (vertex) {
+                pan_pack_ins(c, CS_SLOT, cfg) { cfg.index = 3; }
+                pan_pack_ins(c, CS_WAIT, cfg) { cfg.slots = 1 << 3; }
+                pan_pack_ins(c, CS_HEAPINC, cfg) {
+                        cfg.type = MALI_HEAP_STATISTIC_V_T_START;
+                }
+        } else if (fragment) {
+                pan_pack_ins(c, CS_SLOT, cfg) { cfg.index = 4; }
+                pan_pack_ins(c, CS_WAIT, cfg) { cfg.slots = 1 << 4; }
+        }
+
+        // copying to the main buffer can make debugging easier.
+        // TODO: This needs to be more reliable.
+#if 0
+        unsigned length = (s.ptr - s.begin) * 8;
+        unsigned clamped = MIN2(length, cs->bo->ptr.cpu + cs->bo->size - (void *)c->ptr);
+        memcpy(c->ptr, s->begin, clamped);
+        c->ptr += clamped / 8;
+
+        if (clamped != length) {
+                unsigned rest = length - clamped;
+                c->ptr = cs->bo->ptr.cpu;
+                memcpy(c->ptr, s->begin, rest);
+                c->ptr += rest / 8;
+
+                cs->offset += cs->bo->size;
+        }
+#else
+
+        pan_emit_cs_48(c, 0x48, s.gpu);
+        pan_emit_cs_32(c, 0x4a, (s.ptr - s.begin) * 8);
+        pan_pack_ins(c, CS_CALL, cfg) { cfg.address = 0x48; cfg.length = 0x4a; }
+#endif
+
+        if (vertex) {
+                pan_pack_ins(c, CS_FLUSH_TILER, _) { }
+                pan_pack_ins(c, CS_WAIT, cfg) { cfg.slots = 1 << 3; }
+                pan_pack_ins(c, CS_HEAPINC, cfg) {
+                        cfg.type = MALI_HEAP_STATISTIC_V_T_END;
+                }
+        }
+
+        if (fragment) {
+                /* Skip the next operation if the batch doesn't use a tiler
+                 * heap (i.e. it's just a blit) */
+                pan_emit_cs_ins(c, 22, 0x560030000001); /* b.ne w56, skip 1 */
+                pan_emit_cs_ins(c, 22, 0x570020000007); /* b.eq w57, skip 7 */
+
+                pan_pack_ins(c, CS_LDR, cfg) {
+                        cfg.offset = 4 * 10; /* Heap Start */
+                        cfg.register_mask = 0x3;
+                        cfg.addr = 0x56;
+                        cfg.register_base = 0x4a;
+                }
+                pan_pack_ins(c, CS_LDR, cfg) {
+                        cfg.offset = 4 * 12; /* Heap End */
+                        cfg.register_mask = 0x3;
+                        cfg.addr = 0x56;
+                        cfg.register_base = 0x4c;
+                }
+                pan_pack_ins(c, CS_WAIT, cfg) { cfg.slots = (1 << 0) | (1 << 3); }
+
+                pan_pack_ins(c, CS_HEAPCLEAR, cfg) {
+                        cfg.start = 0x4a;
+                        cfg.end = 0x4c;
+                        cfg.slots = 1 << 3;
+                }
+
+                /* Reset the fields so that the clear operation isn't done again */
+                pan_emit_cs_48(c, 0x4a, 0);
+                pan_pack_ins(c, CS_STR, cfg) {
+                        cfg.offset = 4 * 10; /* Heap Start */
+                        cfg.register_mask = 0x3;
+                        cfg.addr = 0x56;
+                        cfg.register_base = 0x4a;
+                }
+                pan_pack_ins(c, CS_STR, cfg) {
+                        cfg.offset = 4 * 12; /* Heap End */
+                        cfg.register_mask = 0x3;
+                        cfg.addr = 0x56;
+                        cfg.register_base = 0x4a;
+                }
+
+                /* Branch target for above branch */
+
+                // This seems to be done by the HEAPCLEAR
+                //pan_pack_ins(c, CS_HEAPINC, cfg) {
+                //        cfg.type = MALI_HEAP_STATISTIC_FRAGMENT_END;
+                //}
+        }
+
+        if (fragment) {
+                pan_emit_cs_32(c, 0x54, 0);
+                pan_emit_cs_ins(c, 0x24, 0x2540000f80211);
+                pan_pack_ins(c, CS_WAIT, cfg) { cfg.slots = 1 << 1; }
+        }
+
+        {
+                // This could I think be optimised to 0xf80211 rather than 0x233
+                // TODO: Does this need to run for vertex jobs?
+                // What about when doing transform feedback?
+                // I think we at least need it for compute?
+
+                //pan_emit_cs_32(c, 0x54, 0);
+                //pan_emit_cs_ins(c, 0x24, 0x540000000233ULL);
+        }
+
+        if (last) {
+                uint64_t kcpu_seqnum = ++cs->kcpu_seqnum;
+
+                pan_emit_cs_64(c, 0x40, kcpu_seqnum + 1);
+                pan_emit_cs_48(c, 0x42, cs->kcpu_event_ptr);
+                pan_pack_ins(c, CS_EVSTR_64, cfg) {
+                        /* This is the scoreboard mask, right?.. */
+                        cfg.unk_2 = (3 << 3);
+                        cfg.value = 0x40;
+                        cfg.addr = 0x42;
+                }
+
+                dev->mali.kcpu_cqs_wait(&dev->mali, cs->base.ctx,
+                                        cs->kcpu_event_ptr, kcpu_seqnum);
+
+                int fence = dev->mali.kcpu_fence_export(&dev->mali, cs->base.ctx);
+
+                if (fence != -1) {
+                        util_dynarray_foreach(&batch->dmabufs, int, fd) {
+                                panfrost_import_dmabuf_fence(*fd, fence);
+                        }
+                }
+
+                close(fence);
+        }
+
+        pan_emit_cs_48(c, 0x48, cs->event_ptr);
+        pan_emit_cs_64(c, 0x4a, cs->seqnum + 1);
+        pan_pack_ins(c, CS_EVSTR_64, cfg) {
+                /* This is the scoreboard mask, right?.. */
+                cfg.unk_2 = (3 << 3);
+                cfg.value = 0x4a;
+                cfg.addr = 0x48;
+        }
+
+        // TODO: is this just a weird ddk thing, or is it required?
+        // Probably it just lessens the WC impact
+        while ((uintptr_t)c->ptr & 63)
+                pan_emit_cs_ins(c, 0, 0);
+
+        assert(c->ptr <= limit);
+}
+
+static void
+emit_csf_toplevel(struct panfrost_batch *batch)
+{
+        pan_command_stream *cv = &batch->ctx->kbase_cs_vertex.cs;
+        pan_command_stream *cf = &batch->ctx->kbase_cs_fragment.cs;
+
+        pan_command_stream v = batch->cs_vertex;
+        pan_command_stream f = batch->cs_fragment;
+
+        if (batch->cs_vertex_last_size) {
+                assert(v.ptr <= v.end);
+                *batch->cs_vertex_last_size = (v.ptr - v.begin) * 8;
+                v = batch->cs_vertex_first;
+        }
+
+        bool vert = (v.ptr != v.begin);
+        bool frag = (f.ptr != f.begin);
+
+        // TODO: Clean up control-flow?
+
+        if (vert) {
+                pan_emit_cs_48(cv, 0x48, batch->ctx->kbase_ctx->tiler_heap_va);
+                pan_pack_ins(cv, CS_HEAPCTX, cfg) { cfg.address = 0x48; }
+
+                emit_csf_queue(batch, &batch->ctx->kbase_cs_vertex, v,
+                               &batch->vert_deps, true, !frag);
+        }
+
+        if (!frag)
+                return;
+
+        pan_emit_cs_48(cf, 0x48, batch->ctx->kbase_ctx->tiler_heap_va);
+        pan_pack_ins(cf, CS_HEAPCTX, cfg) { cfg.address = 0x48; }
+
+        uint64_t vertex_seqnum = batch->ctx->kbase_cs_vertex.seqnum;
+        // TODO: this assumes SAME_VA
+        mali_ptr seqnum_ptr = (uintptr_t) batch->ctx->kbase_cs_vertex.event_ptr;
+
+        pan_emit_cs_48(cf, 0x4c, seqnum_ptr);
+        pan_emit_cs_64(cf, 0x4e, vertex_seqnum);
+
+        // What does this instruction do?
+        //pan_emit_cs_32(cf, 0x54, 0);
+        //pan_emit_cs_ins(cf, 0x24, 0x540000000200);
+
+        assert(vert || batch->tiler_ctx.bifrost == 0);
+        pan_emit_cs_48(cf, 0x56, batch->tiler_ctx.bifrost);
+
+        emit_csf_queue(batch, &batch->ctx->kbase_cs_fragment, f,
+                       &batch->frag_deps, !vert, true);
+}
+
+static void
+init_cs(struct panfrost_context *ctx, struct panfrost_cs *cs)
+{
+        struct panfrost_device *dev = pan_device(ctx->base.screen);
+        pan_command_stream *c = &cs->cs;
+
+        cs->seqnum = 0;
+
+        cs->offset = 0;
+        c->ptr = cs->bo->ptr.cpu;
+        c->begin = cs->bo->ptr.cpu;
+        c->end = cs->bo->ptr.cpu + cs->base.size;
+        c->gpu = cs->bo->ptr.gpu;
+
+        // eight instructions == 64 bytes
+        pan_pack_ins(c, CS_RESOURCES, cfg) { cfg.mask = cs->hw_resources; }
+        pan_pack_ins(c, CS_SLOT, cfg) { cfg.index = 2; }
+        pan_emit_cs_48(c, 0x48, ctx->kbase_ctx->tiler_heap_va);
+        pan_pack_ins(c, CS_HEAPCTX, cfg) { cfg.address = 0x48; }
+        for (unsigned i = 0; i < 4; ++i)
+                pan_pack_ins(c, CS_NOP, _);
+
+        dev->mali.cs_submit(&dev->mali, &cs->base, 64, NULL, 0);
+        //dev->mali.cs_wait(&dev->mali, &cs->base, 64);
+}
+
+#endif
+
 #define DEFINE_CASE(c) case PIPE_PRIM_##c: return MALI_DRAW_MODE_##c;
 
 static uint8_t
@@ -2904,14 +3315,14 @@ panfrost_draw_emit_vertex(struct panfrost_batch *batch,
 #endif
 
 static void
-panfrost_emit_primitive_size(struct panfrost_context *ctx,
+panfrost_emit_primitive_size(struct panfrost_batch *batch,
                              bool points, mali_ptr size_array,
                              void *prim_size)
 {
-        struct panfrost_rasterizer *rast = ctx->rasterizer;
+        struct panfrost_rasterizer *rast = batch->ctx->rasterizer;
 
-        pan_pack(prim_size, PRIMITIVE_SIZE, cfg) {
-                if (panfrost_writes_point_size(ctx)) {
+        pan_pack_cs_v10(prim_size, &batch->cs_vertex, PRIMITIVE_SIZE, cfg) {
+                if (panfrost_writes_point_size(batch->ctx)) {
                         cfg.size_array = size_array;
                 } else {
                         cfg.constant = points ?
@@ -3037,6 +3448,43 @@ panfrost_update_state_3d(struct panfrost_batch *batch)
 }
 
 #if PAN_ARCH >= 6
+
+#if PAN_ARCH >= 10
+static mali_ptr
+panfrost_get_tiler_heap_desc(struct panfrost_batch *batch)
+{
+        struct panfrost_context *ctx = batch->ctx;
+        struct panfrost_device *dev = pan_device(ctx->base.screen);
+
+        if (ctx->tiler_heap_desc)
+                return ctx->tiler_heap_desc->ptr.gpu;
+
+        ctx->tiler_heap_desc = panfrost_bo_create(dev, 4096, 0, "Tiler heap descriptor");
+
+        pan_pack(ctx->tiler_heap_desc->ptr.cpu, TILER_HEAP, heap) {
+                heap.size = ctx->kbase_ctx->tiler_heap_chunk_size;
+                heap.base = ctx->kbase_ctx->tiler_heap_header;
+                heap.bottom = heap.base + 64;
+                heap.top = heap.base + heap.size;
+        }
+
+        return ctx->tiler_heap_desc->ptr.gpu;
+}
+#else
+static mali_ptr
+panfrost_get_tiler_heap_desc(struct panfrost_batch *batch)
+{
+        struct panfrost_device *dev = pan_device(batch->ctx->base.screen);
+
+        struct panfrost_ptr t =
+                pan_pool_alloc_desc(&batch->pool.base, TILER_HEAP);
+
+        GENX(pan_emit_tiler_heap)(dev, t.cpu);
+
+        return t.gpu;
+}
+#endif
+
 static mali_ptr
 panfrost_batch_get_bifrost_tiler(struct panfrost_batch *batch, unsigned vertex_count)
 {
@@ -3048,18 +3496,32 @@ panfrost_batch_get_bifrost_tiler(struct panfrost_batch *batch, unsigned vertex_c
         if (batch->tiler_ctx.bifrost)
                 return batch->tiler_ctx.bifrost;
 
-        struct panfrost_ptr t =
-                pan_pool_alloc_desc(&batch->pool.base, TILER_HEAP);
+        mali_ptr heap = panfrost_get_tiler_heap_desc(batch);
 
-        GENX(pan_emit_tiler_heap)(dev, t.cpu);
+        mali_ptr scratch = 0;
+
+#if PAN_ARCH >= 10
+        // TODO: Dynamically size?
+        unsigned scratch_bits = 16;
+
+        /* Allocate scratch space for vertex positions / point sizes */
+        // TODO: Should this be shared?
+        struct panfrost_ptr sc =
+                pan_pool_alloc_aligned(&batch->pool.base, 1 << scratch_bits, 4096);
+
+        /* I think the scratch size is passed in the low bits of the
+         * pointer... but trying to go above 16 gives a CS_INHERIT_FAULT.
+         */
+        scratch = sc.gpu + scratch_bits;
+#endif
 
-        mali_ptr heap = t.gpu;
+        struct panfrost_ptr t =
+                pan_pool_alloc_desc(&batch->pool.base, TILER_CONTEXT);
 
-        t = pan_pool_alloc_desc(&batch->pool.base, TILER_CONTEXT);
         GENX(pan_emit_tiler_ctx)(dev, batch->key.width, batch->key.height,
                                  util_framebuffer_get_num_samples(&batch->key),
                                  pan_tristate_get(batch->first_provoking_vertex),
-                                 heap, t.cpu);
+                                 heap, scratch, t.cpu);
 
         batch->tiler_ctx.bifrost = t.gpu;
         return batch->tiler_ctx.bifrost;
@@ -3070,18 +3532,19 @@ panfrost_batch_get_bifrost_tiler(struct panfrost_batch *batch, unsigned vertex_c
  * jobs and Valhall IDVS jobs
  */
 static void
-panfrost_emit_primitive(struct panfrost_context *ctx,
+panfrost_emit_primitive(struct panfrost_batch *batch,
                         const struct pipe_draw_info *info,
                         const struct pipe_draw_start_count_bias *draw,
                         mali_ptr indices, bool secondary_shader, void *out)
 {
-        UNUSED struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
+        struct panfrost_context *ctx = batch->ctx;
+        struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
 
         bool lines = (info->mode == PIPE_PRIM_LINES ||
                       info->mode == PIPE_PRIM_LINE_LOOP ||
                       info->mode == PIPE_PRIM_LINE_STRIP);
 
-        pan_pack(out, PRIMITIVE, cfg) {
+        pan_pack_cs_v10(out, &batch->cs_vertex, PRIMITIVE, cfg) {
                 cfg.draw_mode = pan_draw_mode(info->mode);
                 if (panfrost_writes_point_size(ctx))
                         cfg.point_size_array_format = MALI_POINT_SIZE_ARRAY_FORMAT_FP16;
@@ -3113,12 +3576,20 @@ panfrost_emit_primitive(struct panfrost_context *ctx,
 
                 /* Non-fixed restart indices should have been lowered */
                 assert(!cfg.primitive_restart || panfrost_is_implicit_prim_restart(info));
+
+                /* TODO: This is in a hot function, optimise? */
+                if (ctx->pipe_viewport.scale[2] > 0) {
+                        cfg.low_depth_cull = rast->depth_clip_near;
+                        cfg.high_depth_cull = rast->depth_clip_far;
+                } else {
+                        cfg.low_depth_cull = rast->depth_clip_far;
+                        cfg.high_depth_cull = rast->depth_clip_near;
+                }
 #endif
 
                 cfg.index_count = ctx->indirect_draw ? 1 : draw->count;
                 cfg.index_type = panfrost_translate_index_size(info->index_size);
 
-
                 if (PAN_ARCH >= 9) {
                         /* Base vertex offset on Valhall is used for both
                          * indexed and non-indexed draws, in a simple way for
@@ -3240,7 +3711,7 @@ panfrost_emit_draw(void *out,
         struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
         bool polygon = (prim == PIPE_PRIM_TRIANGLES);
 
-        pan_pack(out, DRAW, cfg) {
+        pan_pack_cs_v10(out, &batch->cs_vertex, DRAW, cfg) {
                 /*
                  * From the Gallium documentation,
                  * pipe_rasterizer_state::cull_face "indicates which faces of
@@ -3270,6 +3741,7 @@ panfrost_emit_draw(void *out,
                         ctx->prog[PIPE_SHADER_FRAGMENT];
 
                 cfg.multisample_enable = rast->multisample;
+
                 cfg.sample_mask = rast->multisample ? ctx->sample_mask : 0xFFFF;
 
                 /* Use per-sample shading if required by API Also use it when a
@@ -3283,7 +3755,10 @@ panfrost_emit_draw(void *out,
 
                 cfg.single_sampled_lines = !rast->multisample;
 
+                /* This is filled in by hardware on v10 */
+#if PAN_ARCH < 10
                 cfg.vertex_array.packet = true;
+#endif
 
                 cfg.minimum_z = batch->minimum_z;
                 cfg.maximum_z = batch->maximum_z;
@@ -3411,14 +3886,18 @@ panfrost_emit_malloc_vertex(struct panfrost_batch *batch,
          */
         secondary_shader &= fs_required;
 
-        panfrost_emit_primitive(ctx, info, draw, 0, secondary_shader,
+#if PAN_ARCH < 10
+        panfrost_emit_primitive(batch, info, draw, 0, secondary_shader,
                                 pan_section_ptr(job, MALLOC_VERTEX_JOB, PRIMITIVE));
+#else
+        panfrost_emit_primitive(batch, info, draw, 0, secondary_shader, job);
+#endif
 
-        pan_section_pack(job, MALLOC_VERTEX_JOB, INSTANCE_COUNT, cfg) {
+        pan_section_pack_cs_v10(job, &batch->cs_vertex, MALLOC_VERTEX_JOB, INSTANCE_COUNT, cfg) {
                 cfg.count = info->instance_count;
         }
 
-        pan_section_pack(job, MALLOC_VERTEX_JOB, ALLOCATION, cfg) {
+        pan_section_pack_cs_v10(job, &batch->cs_vertex, MALLOC_VERTEX_JOB, ALLOCATION, cfg) {
                 if (secondary_shader) {
                         unsigned v = vs->info.varyings.output_count;
                         unsigned f = fs->info.varyings.input_count;
@@ -3427,34 +3906,45 @@ panfrost_emit_malloc_vertex(struct panfrost_batch *batch,
                         unsigned size = slots * 16;
 
                         /* Assumes 16 byte slots. We could do better. */
+#if PAN_ARCH < 10
                         cfg.vertex_packet_stride = size + 16;
+#endif
                         cfg.vertex_attribute_stride = size;
                 } else {
                         /* Hardware requirement for "no varyings" */
+#if PAN_ARCH < 10
                         cfg.vertex_packet_stride = 16;
+#endif
                         cfg.vertex_attribute_stride = 0;
                 }
         }
 
-        pan_section_pack(job, MALLOC_VERTEX_JOB, TILER, cfg) {
+        pan_section_pack_cs_v10(job, &batch->cs_vertex, MALLOC_VERTEX_JOB, TILER, cfg) {
                 cfg.address = panfrost_batch_get_bifrost_tiler(batch, ~0);
         }
 
+        /* For v10, the scissor is emitted directly by
+         * panfrost_emit_viewport */
+#if PAN_ARCH < 10
         STATIC_ASSERT(sizeof(batch->scissor) == pan_size(SCISSOR));
         memcpy(pan_section_ptr(job, MALLOC_VERTEX_JOB, SCISSOR),
                &batch->scissor, pan_size(SCISSOR));
+#endif
 
-        panfrost_emit_primitive_size(ctx, info->mode == PIPE_PRIM_POINTS, 0,
+        panfrost_emit_primitive_size(batch, info->mode == PIPE_PRIM_POINTS, 0,
                                      pan_section_ptr(job, MALLOC_VERTEX_JOB, PRIMITIVE_SIZE));
 
-        pan_section_pack(job, MALLOC_VERTEX_JOB, INDICES, cfg) {
+        pan_section_pack_cs_v10(job, &batch->cs_vertex, MALLOC_VERTEX_JOB, INDICES, cfg) {
                 cfg.address = indices;
+#if PAN_ARCH >= 10
+                cfg.size = draw->count * info->index_size;
+#endif
         }
 
         panfrost_emit_draw(pan_section_ptr(job, MALLOC_VERTEX_JOB, DRAW),
                            batch, fs_required, u_reduced_prim(info->mode), 0, 0, 0);
 
-        pan_section_pack(job, MALLOC_VERTEX_JOB, POSITION, cfg) {
+        pan_section_pack_cs_v10(job, &batch->cs_vertex, MALLOC_VERTEX_JOB, POSITION, cfg) {
                 /* IDVS/points vertex shader */
                 mali_ptr vs_ptr = batch->rsd[PIPE_SHADER_VERTEX];
 
@@ -3464,20 +3954,21 @@ panfrost_emit_malloc_vertex(struct panfrost_batch *batch,
 
                 panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX, vs_ptr,
                                      batch->tls.gpu);
-        }
 
-        pan_section_pack(job, MALLOC_VERTEX_JOB, VARYING, cfg) {
-                /* If a varying shader is used, we configure it with the same
-                 * state as the position shader for backwards compatible
-                 * behaviour with Bifrost. This could be optimized.
-                 */
-                if (!secondary_shader) continue;
+                pan_section_pack_cs_v10(job, &batch->cs_vertex, MALLOC_VERTEX_JOB, VARYING, vary) {
+                        /* If a varying shader is used, we configure it with the same
+                         * state as the position shader for backwards compatible
+                         * behaviour with Bifrost. This could be optimized.
+                         */
+                        if (!secondary_shader) continue;
 
-                mali_ptr ptr = batch->rsd[PIPE_SHADER_VERTEX] +
+                        mali_ptr ptr = batch->rsd[PIPE_SHADER_VERTEX] +
                                 (2 * pan_size(SHADER_PROGRAM));
 
-                panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX,
-                             ptr, batch->tls.gpu);
+                        vary.shader = ptr;
+
+                        // TODO: Fix this function for v9!
+                }
         }
 }
 #endif
@@ -3492,12 +3983,10 @@ panfrost_draw_emit_tiler(struct panfrost_batch *batch,
                          mali_ptr pos, mali_ptr psiz, bool secondary_shader,
                          void *job)
 {
-        struct panfrost_context *ctx = batch->ctx;
-
         void *section = pan_section_ptr(job, TILER_JOB, INVOCATION);
         memcpy(section, invocation_template, pan_size(INVOCATION));
 
-        panfrost_emit_primitive(ctx, info, draw, indices, secondary_shader,
+        panfrost_emit_primitive(batch, info, draw, indices, secondary_shader,
                                 pan_section_ptr(job, TILER_JOB, PRIMITIVE));
 
         void *prim_size = pan_section_ptr(job, TILER_JOB, PRIMITIVE_SIZE);
@@ -3514,7 +4003,7 @@ panfrost_draw_emit_tiler(struct panfrost_batch *batch,
         panfrost_emit_draw(pan_section_ptr(job, TILER_JOB, DRAW),
                            batch, true, prim, pos, fs_vary, varyings);
 
-        panfrost_emit_primitive_size(ctx, prim == PIPE_PRIM_POINTS, psiz, prim_size);
+        panfrost_emit_primitive_size(batch, prim == PIPE_PRIM_POINTS, psiz, prim_size);
 }
 #endif
 
@@ -3526,8 +4015,8 @@ panfrost_launch_xfb(struct panfrost_batch *batch,
 {
         struct panfrost_context *ctx = batch->ctx;
 
-        struct panfrost_ptr t =
-                pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
+        UNUSED struct panfrost_ptr t =
+                pan_pool_alloc_desc_cs_v10(&batch->pool.base, COMPUTE_JOB);
 
         /* Nothing to do */
         if (batch->ctx->streamout.num_targets == 0)
@@ -3556,7 +4045,7 @@ panfrost_launch_xfb(struct panfrost_batch *batch,
         batch->rsd[PIPE_SHADER_VERTEX] = panfrost_emit_compute_shader_meta(batch, PIPE_SHADER_VERTEX);
 
 #if PAN_ARCH >= 9
-        pan_section_pack(t.cpu, COMPUTE_JOB, PAYLOAD, cfg) {
+        pan_section_pack_cs_v10(t.cpu, &batch->cs_vertex, COMPUTE_JOB, PAYLOAD, cfg) {
                 cfg.workgroup_size_x = 1;
                 cfg.workgroup_size_y = 1;
                 cfg.workgroup_size_z = 1;
@@ -3569,15 +4058,20 @@ panfrost_launch_xfb(struct panfrost_batch *batch,
                                      batch->rsd[PIPE_SHADER_VERTEX],
                                      batch->tls.gpu);
 
+#if PAN_ARCH < 10
                 /* TODO: Indexing. Also, this is a legacy feature... */
                 cfg.compute.attribute_offset = batch->ctx->offset_start;
+#endif
 
                 /* Transform feedback shaders do not use barriers or shared
                  * memory, so we may merge workgroups.
                  */
                 cfg.allow_merging_workgroups = true;
+
+#if PAN_ARCH < 10
                 cfg.task_increment = 1;
                 cfg.task_axis = MALI_TASK_AXIS_Z;
+#endif
         }
 #else
         struct mali_invocation_packed invocation;
@@ -3593,12 +4087,20 @@ panfrost_launch_xfb(struct panfrost_batch *batch,
         panfrost_draw_emit_vertex(batch, info, &invocation, 0, 0,
                                   attribs, attrib_bufs, t.cpu);
 #endif
+#if PAN_ARCH >= 10
+        // TODO: Use a seperate compute queue?
+        pan_pack_ins(&batch->cs_vertex, COMPUTE_LAUNCH, cfg) {
+                // TODO v10: Set parameters
+        }
+        batch->scoreboard.first_job = 1;
+#else
         enum mali_job_type job_type = MALI_JOB_TYPE_COMPUTE;
 #if PAN_ARCH <= 5
         job_type = MALI_JOB_TYPE_VERTEX;
 #endif
         panfrost_add_job(&batch->pool.base, &batch->scoreboard, job_type,
                          true, false, 0, 0, &t, false);
+#endif
 
         ctx->uncompiled[PIPE_SHADER_VERTEX] = vs_uncompiled;
         ctx->prog[PIPE_SHADER_VERTEX] = vs;
@@ -3607,6 +4109,54 @@ panfrost_launch_xfb(struct panfrost_batch *batch,
         batch->push_uniforms[PIPE_SHADER_VERTEX] = saved_push;
 }
 
+#if PAN_ARCH >= 10
+static pan_command_stream
+panfrost_batch_create_cs(struct panfrost_batch *batch, unsigned count)
+{
+        struct panfrost_ptr cs = pan_pool_alloc_aligned(&batch->pool.base, count * 8, 64);
+
+        return (pan_command_stream) {
+                .ptr = cs.cpu,
+                .begin = cs.cpu,
+                .end = cs.cpu + count,
+                .gpu = cs.gpu,
+        };
+}
+
+static uint64_t *
+panfrost_cs_vertex_allocate_instrs(struct panfrost_batch *batch, unsigned count)
+{
+        /* Doing a tail call to another buffer takes three instructions */
+        count += 3;
+
+        pan_command_stream v = batch->cs_vertex;
+
+        if (v.ptr + count > v.end) {
+                batch->cs_vertex = panfrost_batch_create_cs(batch, MAX2(count, 1 << 13));
+
+                /* The size will be filled in later. */
+                uint32_t *last_size = (uint32_t *)v.ptr;
+                pan_emit_cs_32(&v, 0x5e, 0);
+
+                pan_emit_cs_48(&v, 0x5c, batch->cs_vertex.gpu);
+                pan_pack_ins(&v, CS_TAILCALL, cfg) { cfg.address = 0x5c; cfg.length = 0x5e; }
+
+                assert(v.ptr <= v.end);
+
+                /* This is not strictly required, but makes disassembly look
+                 * nicer */
+                if (batch->cs_vertex_last_size)
+                        *batch->cs_vertex_last_size = (v.ptr - v.begin) * 8;
+
+                batch->cs_vertex_last_size = last_size;
+                if (!batch->cs_vertex_first.gpu)
+                        batch->cs_vertex_first = v;
+        }
+
+        return batch->cs_vertex.ptr + count;
+}
+#endif
+
 static void
 panfrost_direct_draw(struct panfrost_batch *batch,
                      const struct pipe_draw_info *info,
@@ -3618,6 +4168,11 @@ panfrost_direct_draw(struct panfrost_batch *batch,
 
         struct panfrost_context *ctx = batch->ctx;
 
+#if PAN_ARCH >= 10
+        /* TODO: We don't need quite so much space */
+        uint64_t *limit = panfrost_cs_vertex_allocate_instrs(batch, 64);
+#endif
+
         /* If we change whether we're drawing points, or whether point sprites
          * are enabled (specified in the rasterizer), we may need to rebind
          * shaders accordingly. This implicitly covers the case of rebinding
@@ -3647,18 +4202,19 @@ panfrost_direct_draw(struct panfrost_batch *batch,
 
         UNUSED struct panfrost_ptr tiler, vertex;
 
-        if (idvs) {
 #if PAN_ARCH >= 9
-                tiler = pan_pool_alloc_desc(&batch->pool.base, MALLOC_VERTEX_JOB);
-#elif PAN_ARCH >= 6
+        tiler = pan_pool_alloc_desc_cs_v10(&batch->pool.base, MALLOC_VERTEX_JOB);
+#else /* PAN_ARCH < 9 */
+        if (idvs) {
+#if PAN_ARCH >= 6
                 tiler = pan_pool_alloc_desc(&batch->pool.base, INDEXED_VERTEX_JOB);
-#else
-                unreachable("IDVS is unsupported on Midgard");
 #endif
+                unreachable("IDVS is unsupported on Midgard");
         } else {
-                vertex = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
-                tiler = pan_pool_alloc_desc(&batch->pool.base, TILER_JOB);
+                vertex = pan_pool_alloc_desc_cs_v10(&batch->pool.base, COMPUTE_JOB);
+                tiler = pan_pool_alloc_desc_cs_v10(&batch->pool.base, TILER_JOB);
         }
+#endif /* PAN_ARCH */
 
         unsigned vertex_count = ctx->vertex_count;
 
@@ -3726,7 +4282,7 @@ panfrost_direct_draw(struct panfrost_batch *batch,
 
         mali_ptr attribs, attrib_bufs;
         attribs = panfrost_emit_vertex_data(batch, &attrib_bufs);
-#endif
+#endif /* PAN_ARCH <= 7 */
 
         panfrost_update_state_3d(batch);
         panfrost_update_shader_state(batch, PIPE_SHADER_VERTEX);
@@ -3752,13 +4308,25 @@ panfrost_direct_draw(struct panfrost_batch *batch,
 #if PAN_ARCH >= 9
         assert(idvs && "Memory allocated IDVS required on Valhall");
 
-        panfrost_emit_malloc_vertex(batch, info, draw, indices,
-                                    secondary_shader, tiler.cpu);
+        panfrost_emit_malloc_vertex(batch, info, draw, indices, secondary_shader, tiler.cpu);
 
+#if PAN_ARCH >= 10
+        pan_pack_ins(&batch->cs_vertex, IDVS_LAUNCH, _);
+        /* TODO: Find a better way to specify that there were jobs */
+        batch->scoreboard.first_job = 1;
+        batch->scoreboard.first_tiler = NULL + 1;
+
+        /* Make sure we didn't use more CS instructions than we allocated
+         * space for */
+        assert(batch->cs_vertex.ptr <= limit);
+
+#else /* PAN_ARCH < 10 */
         panfrost_add_job(&batch->pool.base, &batch->scoreboard,
                          MALI_JOB_TYPE_MALLOC_VERTEX, false, false, 0,
                          0, &tiler, false);
-#else
+#endif
+#else /* PAN_ARCH < 9 */
+
         /* Fire off the draw itself */
         panfrost_draw_emit_tiler(batch, info, draw, &invocation, indices,
                                  fs_vary, varyings, pos, psiz, secondary_shader,
@@ -3773,7 +4341,7 @@ panfrost_direct_draw(struct panfrost_batch *batch,
                 panfrost_add_job(&batch->pool.base, &batch->scoreboard,
                                  MALI_JOB_TYPE_INDEXED_VERTEX, false, false,
                                  0, 0, &tiler, false);
-#endif
+#endif /* PAN_ARCH < 6 */
         } else {
                 panfrost_draw_emit_vertex(batch, info, &invocation,
                                           vs_vary, varyings, attribs, attrib_bufs, vertex.cpu);
@@ -4102,8 +4670,8 @@ panfrost_launch_grid(struct pipe_context *pipe,
 
         ctx->compute_grid = info;
 
-        struct panfrost_ptr t =
-                pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
+        UNUSED struct panfrost_ptr t =
+                pan_pool_alloc_desc_cs_v10(&batch->pool.base, COMPUTE_JOB);
 
         /* Invoke according to the grid info */
 
@@ -4143,7 +4711,7 @@ panfrost_launch_grid(struct pipe_context *pipe,
 #else
         struct panfrost_compiled_shader *cs = ctx->prog[PIPE_SHADER_COMPUTE];
 
-        pan_section_pack(t.cpu, COMPUTE_JOB, PAYLOAD, cfg) {
+        pan_section_pack_cs_v10(t.cpu, &batch->cs_vertex, COMPUTE_JOB, PAYLOAD, cfg) {
                 cfg.workgroup_size_x = info->block[0];
                 cfg.workgroup_size_y = info->block[1];
                 cfg.workgroup_size_z = info->block[2];
@@ -4166,12 +4734,14 @@ panfrost_launch_grid(struct pipe_context *pipe,
                         cs->info.cs.allow_merging_workgroups &&
                         (info->variable_shared_mem == 0);
 
+#if PAN_ARCH < 10
                 cfg.task_increment = 1;
                 cfg.task_axis = MALI_TASK_AXIS_Z;
+#endif
         }
 #endif
 
-        unsigned indirect_dep = 0;
+        UNUSED unsigned indirect_dep = 0; // TODO v10 (unused)
 #if PAN_GPU_INDIRECTS
         if (info->indirect) {
                 struct pan_indirect_dispatch_info indirect = {
@@ -4191,9 +4761,17 @@ panfrost_launch_grid(struct pipe_context *pipe,
         }
 #endif
 
+#if PAN_ARCH >= 10
+        pan_pack_ins(&batch->cs_vertex, COMPUTE_LAUNCH, cfg) {
+                /* TODO: Change this as needed */
+                cfg.unk_1 = 512;
+        }
+        batch->scoreboard.first_job = 1;
+#else
         panfrost_add_job(&batch->pool.base, &batch->scoreboard,
                          MALI_JOB_TYPE_COMPUTE, true, false,
                          indirect_dep, 0, &t, false);
+#endif
         panfrost_flush_all_batches(ctx, "Launch grid post-barrier");
 }
 
@@ -4453,6 +5031,30 @@ panfrost_create_sampler_view(
         return (struct pipe_sampler_view *) so;
 }
 
+static void
+panfrost_init_logicop_blend_state(struct panfrost_blend_state *so)
+{
+        for (unsigned c = 0; c < so->pan.rt_count; ++c) {
+                unsigned g = so->base.independent_blend_enable ? c : 0;
+                const struct pipe_rt_blend_state pipe = so->base.rt[g];
+
+                struct pan_blend_equation equation = {0};
+
+                equation.color_mask = pipe.colormask;
+                equation.blend_enable = false;
+
+                so->info[c] = (struct pan_blend_info) {
+                        .enabled = (pipe.colormask != 0),
+                        .load_dest = true,
+                        .fixed_function = false,
+                };
+
+                so->pan.rts[c].equation = equation;
+
+                so->load_dest_mask |= BITFIELD_BIT(c);
+        }
+}
+
 /* A given Gallium blend state can be encoded to the hardware in numerous,
  * dramatically divergent ways due to the interactions of blending with
  * framebuffer formats. Conceptually, there are two modes:
@@ -4492,6 +5094,11 @@ panfrost_create_blend_state(struct pipe_context *pipe,
         so->pan.logicop_func = blend->logicop_func;
         so->pan.rt_count = blend->max_rt + 1;
 
+        if (blend->logicop_enable) {
+                panfrost_init_logicop_blend_state(so);
+                return so;
+        }
+
         for (unsigned c = 0; c < so->pan.rt_count; ++c) {
                 unsigned g = blend->independent_blend_enable ? c : 0;
                 const struct pipe_rt_blend_state pipe = blend->rt[g];
@@ -4521,12 +5128,10 @@ panfrost_create_blend_state(struct pipe_context *pipe,
                         .opaque = pan_blend_is_opaque(equation),
                         .constant_mask = constant_mask,
 
-                        /* TODO: check the dest for the logicop */
-                        .load_dest = blend->logicop_enable ||
-                                pan_blend_reads_dest(equation),
+                        .load_dest = pan_blend_reads_dest(equation),
 
                         /* Could this possibly be fixed-function? */
-                        .fixed_function = !blend->logicop_enable &&
+                        .fixed_function =
                                 pan_blend_can_fixed_function(equation,
                                                              supports_2src) &&
                                 (!constant_mask ||
@@ -4612,10 +5217,12 @@ prepare_shader(struct panfrost_compiled_shader *state,
 
         state->state = panfrost_pool_take_ref(pool, ptr.gpu);
 
+        // TODO: Why set primary_shader to false again?
+
         /* Generic, or IDVS/points */
         pan_pack(ptr.cpu, SHADER_PROGRAM, cfg) {
                 cfg.stage = pan_shader_stage(&state->info);
-                cfg.primary_shader = true;
+                cfg.primary_shader = false;
                 cfg.register_allocation = pan_register_allocation(state->info.work_reg_count);
                 cfg.binary = state->bin.gpu;
                 cfg.preload.r48_r63 = (state->info.preload >> 48);
@@ -4631,7 +5238,7 @@ prepare_shader(struct panfrost_compiled_shader *state,
         /* IDVS/triangles */
         pan_pack(ptr.cpu + pan_size(SHADER_PROGRAM), SHADER_PROGRAM, cfg) {
                 cfg.stage = pan_shader_stage(&state->info);
-                cfg.primary_shader = true;
+                cfg.primary_shader = false;
                 cfg.register_allocation = pan_register_allocation(state->info.work_reg_count);
                 cfg.binary = state->bin.gpu + state->info.vs.no_psiz_offset;
                 cfg.preload.r48_r63 = (state->info.preload >> 48);
@@ -4707,6 +5314,11 @@ init_batch(struct panfrost_batch *batch)
         /* On Midgard, the TLS is embedded in the FB descriptor */
         batch->tls = batch->framebuffer;
 #endif
+
+#if PAN_ARCH >= 10
+        batch->cs_vertex = panfrost_batch_create_cs(batch, 1 << 13);
+        batch->cs_fragment = panfrost_batch_create_cs(batch, 1 << 9);
+#endif
 }
 
 static void
@@ -4821,6 +5433,10 @@ GENX(panfrost_cmdstream_screen_init)(struct panfrost_screen *screen)
         screen->vtbl.init_polygon_list = init_polygon_list;
         screen->vtbl.get_compiler_options = GENX(pan_shader_get_compiler_options);
         screen->vtbl.compile_shader = GENX(pan_shader_compile);
+#if PAN_ARCH >= 10
+        screen->vtbl.emit_csf_toplevel = emit_csf_toplevel;
+        screen->vtbl.init_cs = init_cs;
+#endif
 
         GENX(pan_blitter_init)(dev, &screen->blitter.bin_pool.base,
                                &screen->blitter.desc_pool.base);
